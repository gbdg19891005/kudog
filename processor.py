import re
import logging
from functools import lru_cache  # ç¼“å­˜è£…é¥°å™¨ï¼Œæå‡é‡å¤åç§°å¤„ç†é€Ÿåº¦
from typing import Dict, List, Tuple, Optional, Iterator

# === é¢„ç¼–è¯‘æ­£åˆ™ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼Œé‡å¤ä½¿ç”¨ä¸é‡æ–°ç¼–è¯‘ï¼‰ ===
EXTINF_PATTERN = re.compile(r'#EXTINF[:\-]?\d+.*?(?:tvg-name"?="?([^",]+)|(?:,)([^,]+?)(?:$|,))', re.IGNORECASE)
TVGID_PATTERN = re.compile(r'tvg-id"?="?([^",]+)', re.IGNORECASE)
GROUP_PATTERN = re.compile(r'group-title="?([^",]+)', re.IGNORECASE)

@lru_cache(maxsize=1024)  # ç¼“å­˜æœ€è¿‘1024ä¸ªåç§°åŒ¹é…ç»“æœ
def normalize_name(name: str, aliasmap: Dict[str, str]) -> str:
    """åç§°è§„èŒƒåŒ–ï¼šåˆ«åâ†’ä¸»åï¼Œæ”¯æŒæ­£åˆ™åŒ¹é…
    ä¾‹ï¼š'CCTV1 HD' â†’ 'CCTV-1'ï¼ˆalias.txté…ç½®ï¼‰
    """
    for alias, main in aliasmap.items():
        if alias.startswith('re:'):  # æ­£åˆ™åŒ¹é…
            if re.search(alias[3:], name, re.IGNORECASE):
                return main
        elif alias.lower() == name.lower():  # ç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            return main
    return name  # æ— åŒ¹é…è¿”å›åŸå

def assign_group(name: str, rules: Dict[str, List[str]], default_group: str) -> str:
    """æ ¹æ®groups.jsonè§„åˆ™åˆ†é…åˆ†ç»„
    ä¼˜å…ˆçº§ï¼šç¬¬ä¸€ä¸ªåŒ¹é…çš„è§„åˆ™ç»„ â†’ é»˜è®¤ç»„'æœªåˆ†ç±»'
    """
    for group, keywords in rules.items():  # éå†æ‰€æœ‰åˆ†ç»„è§„åˆ™
        for kw in keywords:  # æ¯ä¸ªç»„å¯èƒ½å¤šä¸ªå…³é”®è¯
            try:
                if re.search(kw, name, re.IGNORECASE):  # æ­£åˆ™åŒ¹é…
                    return group
            except re.error:  # æ­£åˆ™è¯­æ³•é”™è¯¯é™çº§ä¸ºå­—ç¬¦ä¸²åŒ¹é…
                if kw.lower() in name.lower():
                    return group
    return default_group

def is_blocked(name: str, blocklist: List[str]) -> bool:
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åœ¨é»‘åå•ï¼ŒåŒ…å«å³å±è”½"""
    clean_name = name.strip()
    if not clean_name:
        return True  # ç©ºåç§°ç›´æ¥å±è”½
    for kw in blocklist:
        if not kw:
            continue
        try:
            if re.search(re.escape(kw.strip()), clean_name, re.IGNORECASE):
                return True
        except re.error:
            if kw.strip().lower() in clean_name.lower():
                return True
    return False

def convert_txt_to_m3u(lines: List[str], default_group: str) -> List[str]:
    """TXTæ ¼å¼è½¬æ ‡å‡†M3Uï¼šå,URL â†’ #EXTINF+URL"""
    new_lines = ['#EXTM3U']
    for line in lines:
        if not line.strip() or line.startswith('#'):
            continue
        try:
            name, url = line.split(',', 1)  # æŒ‰ç¬¬ä¸€ä¸ªé€—å·åˆ†å‰²
            name, url = name.strip(), url.strip()
            # ç”Ÿæˆæ ‡å‡†EXTINFè¡Œ
            new_lines.append(f'#EXTINF:-1 tvg-id="{name}" tvg-name="{name}" group-title="{default_group}",{name}')
            new_lines.append(url)
        except ValueError:  # åˆ†å‰²å¤±è´¥è·³è¿‡
            continue
    return new_lines

def process_channel_pair(line: str, url_line: str, aliasmap: Dict[str, str], 
                        rules: Dict[str, List[str]], blocklist: List[str], 
                        default_group: str, whitelist: Optional[List[str]] = None,
                        source_name: str = "", primary: bool = False) -> Optional[Tuple[str, str, List[str], str]]:
    """å¤„ç†å•ä¸ªEXTINF+URLå¯¹ï¼Œè¿”å›å¤„ç†ç»“æœæˆ–Noneï¼ˆè¿‡æ»¤æ‰ï¼‰"""
    # 1. éªŒè¯URLæœ‰æ•ˆæ€§
    if not url_line or url_line.startswith('#EXTINF'):
        logging.warning(f"âš  MISSING URL {source_name}: {line.strip()}")
        return None
    
    # 2. æ ‡å‡†åŒ–æ—§æ ‡ç­¾å
    line = line.replace('svg-name', 'tvg-name').replace('svg-id', 'tvg-id')
    
    # 3. æå–åŸå§‹é¢‘é“åï¼ˆå¤šç§æ ¼å¼å…¼å®¹ï¼‰
    m = EXTINF_PATTERN.search(line)
    raw_name = m.group(1).strip() if m and m.group(1) else None
    if not raw_name:
        parts = line.split(',', 1)
        raw_name = parts[1].strip() if len(parts) > 1 else ''
    m2 = TVGID_PATTERN.search(line)
    raw_name = m2.group(1).strip() if m2 and not raw_name else raw_name
    
    # 4. åç§°è§„èŒƒåŒ–
    norm_name = normalize_name(raw_name, aliasmap)
    
    # 5. ç™½åå•è¿‡æ»¤ï¼ˆæŒ‡å®šæºåªå–åŒ…å«é¢‘é“ï¼‰
    if whitelist and not any(re.search(kw, norm_name, re.IGNORECASE) for kw in whitelist):
        logging.info(f"â­ FILTERED {source_name}: {raw_name} â†’ {norm_name}")
        return None
    
    # 6. é»‘åå•è¿‡æ»¤
    if is_blocked(norm_name, blocklist):
        logging.info(f"ğŸš« BLOCKED {source_name}: {raw_name} â†’ {norm_name}")
        return None
    
    # 7. åˆ†é…åˆ†ç»„
    group = assign_group(norm_name, rules, default_group)
    
    # 8. å¼ºåˆ¶æ ‡å‡†åŒ–æ ‡ç­¾
    if 'tvg-id' not in line:
        line = re.sub(r'tvg-name"?="?([^",]+)', f'tvg-id="{norm_name}" tvg-name="\\g<1>"', line)
    
    # 9. æ›´æ–°group-title
    line = re.sub(r'group-title="?([^",]+)', '', line)  # æ¸…é™¤æ—§åˆ†ç»„
    if ',' in line:
        parts = line.split(',', 1)
        line = f"{parts[0]}, group-title=\"{group}\", {parts[1]}"
    else:
        line = f"{line}, group-title=\"{group}\""
    
    return norm_name, line, [url_line], group  # è¿”å›è§„èŒƒåŒ–ç»“æœ

def process_lines(lines: List[str], aliasmap: Dict[str, str], rules: Dict[str, List[str]], 
                 blocklist: List[str], keep_multiple_urls: bool, channels: Dict[str, Any],
                 primary: bool = False, source_name: str = "", default_group: str = "",
                 whitelist: Optional[List[str]] = None) -> None:
    """æ‰¹é‡å¤„ç†M3Uè¡Œï¼Œæ¯2è¡Œ(EXTINF+URL)ä¸ºä¸€ç»„"""
    i = 0
    processed = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('#EXTINF'):  # æ‰¾åˆ°é¢‘é“è¡Œ
            url_line = lines[i+1].strip() if i+1 < len(lines) else ''
            result = process_channel_pair(line, url_line, aliasmap, rules, blocklist, 
                                        default_group, whitelist, source_name, primary)
            if result:  # æœ‰æ•ˆé¢‘é“
                norm_name, proc_line, urls, group = result
                if norm_name not in channels:  # æ–°é¢‘é“
                    channels[norm_name] = {'line': proc_line, 'urls': urls, 'group': group}
                    logging.debug(f"â• ADD {source_name}: {norm_name} [{group}]")
                elif primary and urls[0] not in channels[norm_name]['urls']:  # ä¸»æºä¼˜å…ˆ
                    if keep_multiple_urls:
                        channels[norm_name]['urls'].extend(urls)
                        logging.debug(f"ğŸ”— APPEND URL to {norm_name}")
                    else:
                        logging.debug(f"â­ SKIP duplicate URL for {norm_name}")
                else:  # æ¬¡æºé‡å¤ï¼Œè·³è¿‡
                    logging.debug(f"â­ SKIP {source_name}: {norm_name} (exists)")
                
                if group == default_group:
                    logging.warning(f"âš  UNCATEGORIZED: {norm_name}")
                processed += 1
            i += 2  # è·³è¿‡EXTINF+URLä¸¤è¡Œ
        else:
            i += 1  # è·³è¿‡æ³¨é‡Š/ç©ºè¡Œ
    logging.info(f"ğŸ“ˆ Processed {processed} channels from {source_name}")
